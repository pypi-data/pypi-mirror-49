

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>rankeval.analysis package &mdash; RankEval 0.00 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="rankeval.dataset package" href="rankeval.dataset.html" />
    <link rel="prev" title="rankeval package" href="rankeval.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> RankEval
          

          
          </a>

          
            
            
              <div class="version">
                0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="rankeval.html">rankeval package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="rankeval.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">rankeval.analysis package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-rankeval.analysis.effectiveness">rankeval.analysis.effectiveness module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-rankeval.analysis.feature">rankeval.analysis.feature module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-rankeval.analysis.statistical">rankeval.analysis.statistical module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-rankeval.analysis.topological">rankeval.analysis.topological module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="rankeval.dataset.html">rankeval.dataset package</a></li>
<li class="toctree-l3"><a class="reference internal" href="rankeval.metrics.html">rankeval.metrics package</a></li>
<li class="toctree-l3"><a class="reference internal" href="rankeval.model.html">rankeval.model package</a></li>
<li class="toctree-l3"><a class="reference internal" href="rankeval.scoring.html">rankeval.scoring package</a></li>
<li class="toctree-l3"><a class="reference internal" href="rankeval.visualization.html">rankeval.visualization package</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RankEval</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="rankeval.html">rankeval package</a> &raquo;</li>
        
      <li>rankeval.analysis package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rankeval.analysis.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-rankeval.analysis">
<span id="rankeval-analysis-package"></span><h1>rankeval.analysis package<a class="headerlink" href="#module-rankeval.analysis" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference internal" href="#module-rankeval.analysis" title="rankeval.analysis"><code class="xref py py-mod docutils literal notranslate"><span class="pre">rankeval.analysis</span></code></a> module implements the functionalities for analysing the
behaviour of several ranking models with respect to several metrics and
datasets. It proposes a comprehensive set of analysis for tuning, evaluating
and comparing Gradient Boosted Regression Tree models devoted to learning a
ranking function.</p>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-rankeval.analysis.effectiveness">
<span id="rankeval-analysis-effectiveness-module"></span><h2>rankeval.analysis.effectiveness module<a class="headerlink" href="#module-rankeval.analysis.effectiveness" title="Permalink to this headline">¶</a></h2>
<p>This package implements several effectiveness analysis focused on assessing
the performance of the models in terms of accuracy. These functionalities can
be applied to several models at the same time, so to have a direct comparison
of the analysis performed.</p>
<dl class="function">
<dt id="rankeval.analysis.effectiveness.document_graded_relevance">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">document_graded_relevance</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>bins=100</em>, <em>start=None</em>, <em>end=None</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#document_graded_relevance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.document_graded_relevance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the analysis of the model on a per-label basis,
i.e., it allows the evaluation of the cumulative predicted score
distribution. For example, for each relevance label available in each
dataset, it provides the fraction of documents with a predicted score
smaller than a given score (the latter are binned among start and end).
By plotting this fractions it is possible to obtains a curve for each
relevance label. The bigger the distance amongst curves the larger the
model’s discriminative power.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>bins <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>Number of equi-spaced bins for which to computer the cumulative
distribution of the predicted scores. If bin is None, it will use
the maximum number of queries across all the datasets as bins value.</dd>
<dt>start <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>The start point of the range for which we will compute the cumulative
distribution of the predicted scores. If start is None, it will use
the minimum metric score as starting point for the range.</dd>
<dt>end <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>The end point of the range for which we will compute the cumulative
distribution of the predicted scores. If end is None, it will use
the maximum metric score as starting point for the range.</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>graded_relevance <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the fraction of documents with a predicted score
smaller than a given score, for each model and each dataset.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.model_performance">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">model_performance</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>metrics</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#model_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.model_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the model performance analysis (part of the
effectiveness analysis category).</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given metrics and models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>metrics <span class="classifier-delimiter">:</span> <span class="classifier">list of Metric</span></dt>
<dd>The metrics to use for the analysis</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>metric_scores <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the metric scores of the models using
the given metrics on the given datasets.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.query_class_performance">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">query_class_performance</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>metrics</em>, <em>query_classes</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#query_class_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.query_class_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the analysis of the effectiveness of a given model
by providing a breakdown of the performance over query class. Whenever
a query classification is provided, e.g., navigational, informational,
transactional, number of terms composing the query, etc., it provides
the model effectiveness over such classes. This analysis is important
especially in a production environment, as it allows to calibrate the
ranking infrastructure w.r.t. a specific context.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given metrics and models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>metrics <span class="classifier-delimiter">:</span> <span class="classifier">list of Metric</span></dt>
<dd>The metrics to use for the analysis</dd>
<dt>query_classes <span class="classifier-delimiter">:</span> <span class="classifier">list of lists</span></dt>
<dd>A list containing lists of classes each one for a specific Dataset.
The i-th item in the j-th list identifies the class of the i-th query
of the j-th Dataset.</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>query_class_performance <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the per-class metric scores of each model using
the given metrics on the given datasets.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.query_wise_performance">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">query_wise_performance</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>metrics</em>, <em>bins=None</em>, <em>start=None</em>, <em>end=None</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#query_wise_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.query_wise_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the analysis of the model on a query-wise basis,
i.e., it compute the cumulative distribution of a given performance metric.
For example, the fraction of queries with a NDCG score smaller that any
given threshold, over the set of queries described in the dataset.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given metrics and models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>metrics <span class="classifier-delimiter">:</span> <span class="classifier">list of Metric</span></dt>
<dd>The metrics to use for the analysis</dd>
<dt>bins <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>Number of equi-spaced bins for which to computer the cumulative
distribution of the given metric. If bin is None, it will use
the maximum number of queries across all the datasets as bins value.</dd>
<dt>start <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>The start point of the range for which we will compute the cumulative
distribution of the given metric. If start is None, it will use
the minimum metric score as starting point for the range.</dd>
<dt>end <span class="classifier-delimiter">:</span> <span class="classifier">int or None</span></dt>
<dd>The end point of the range for which we will compute the cumulative
distribution of the given metric. If end is None, it will use
the maximum metric score as starting point for the range.</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>metric_scores <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the metric scores of each model using the given
metrics on the given datasets. The metric scores are cumulatively
reported tree by tree, i.e., top 10 trees, top 20, etc., with a step-size
between the number of trees as highlighted by the step parameter.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.rank_confusion_matrix">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">rank_confusion_matrix</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>skip_same_label=False</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#rank_confusion_matrix"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.rank_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>RankEval allows for a novel rank-oriented confusion matrix by reporting for
any given relevance label  l_i, the number of document with a predicted
score smaller than documents with label l_j. When  l_i &gt; l_j this
corresponds to the number of mis-ranked document pairs. This can be
considered as a breakdown over the relevance labels of the ranking
effectiveness of the model.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>skip_same_label <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>True if the method has to skip the pair with the same labels,
False otherwise</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>ranked_matrix: xarray.DataArray</dt>
<dd>A DataArray reporting for any given relevance label  l_i, the number of
documents with a predicted score smaller than documents with label l_j</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.tree_wise_average_contribution">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">tree_wise_average_contribution</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#tree_wise_average_contribution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.tree_wise_average_contribution" title="Permalink to this definition">¶</a></dt>
<dd><p>This method provides the average contribution given by each tree of each
model to the scoring of the datasets.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given metrics and models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>average_contribution <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the average contribution given by each tree of
each model to the scoring of the given datasets. The average
contribution are reported tree by tree.</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.effectiveness.tree_wise_performance">
<code class="descclassname">rankeval.analysis.effectiveness.</code><code class="descname">tree_wise_performance</code><span class="sig-paren">(</span><em>datasets</em>, <em>models</em>, <em>metrics</em>, <em>step=10</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/effectiveness.html#tree_wise_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.effectiveness.tree_wise_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the analysis of the model on a tree-wise basis
(part of the effectiveness analysis category).</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using
the given metrics and models</dd>
<dt>models <span class="classifier-delimiter">:</span> <span class="classifier">list of RTEnsemble</span></dt>
<dd>The models to analyze</dd>
<dt>metrics <span class="classifier-delimiter">:</span> <span class="classifier">list of Metric</span></dt>
<dd>The metrics to use for the analysis</dd>
<dt>step <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Step-size identifying evenly spaced number of trees for evaluating
the top=k model performance.
(e.g., step=100 means the method will evaluate the model performance
at 100, 200, 300, etc trees).</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>metric_scores <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the metric scores of each model using the given
metrics on the given datasets.
The metric scores are cumulatively reported tree by tree, i.e., top 10
trees, top 20, etc., with a step-size between the number of trees
as highlighted by the step parameter.</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-rankeval.analysis.feature">
<span id="rankeval-analysis-feature-module"></span><h2>rankeval.analysis.feature module<a class="headerlink" href="#module-rankeval.analysis.feature" title="Permalink to this headline">¶</a></h2>
<p>This package implements feature importance analysis.</p>
<dl class="function">
<dt id="rankeval.analysis.feature.feature_importance">
<code class="descclassname">rankeval.analysis.feature.</code><code class="descname">feature_importance</code><span class="sig-paren">(</span><em>model</em>, <em>dataset</em>, <em>metric=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/feature.html#feature_importance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.feature.feature_importance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes the feature importance relative to the given model
and dataset.</p>
<dl class="docutils">
<dt>dataset <span class="classifier-delimiter">:</span> <span class="classifier">Dataset</span></dt>
<dd>The dataset used to evaluate the model (typically the one used to train
the model).</dd>
<dt>model <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>The model whose features we want to evaluate.</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">rankeval.metrics.Metric</span></dt>
<dd>The metric to use for compute the feature gain at each split node.
The default metric is the Root Mean Squared Error (MSE).</dd>
</dl>
<dl class="docutils">
<dt>feature_importance <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd><p class="first">A DataArray containing the feature importance scores, one for each
feature of the given model scored on the given dataset. Two main
information are stored in the DataArray:</p>
<blockquote class="last">
<div><ul class="simple">
<li><dl class="first docutils">
<dt>feature_importance: A vector of importance values, one for each</dt>
<dd>feature in the given model. The importance values reported are
the sum of the improvements, in terms of MSE, of each feature,
evaluated on the given dataset. The improvements are computed as
the delta MSE before a split node and after, evaluating how much
the MSE is improved as a result of the split.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>feature_count: A vector of count values, one for each feature in</dt>
<dd>the given model. The count values reported highlights the number
of times each feature is used in a split node, i.e., to improve
the MSE.</dd>
</dl>
</li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-rankeval.analysis.statistical">
<span id="rankeval-analysis-statistical-module"></span><h2>rankeval.analysis.statistical module<a class="headerlink" href="#module-rankeval.analysis.statistical" title="Permalink to this headline">¶</a></h2>
<p>This package implements several statistical significance tests.</p>
<dl class="function">
<dt id="rankeval.analysis.statistical.bias_variance">
<code class="descclassname">rankeval.analysis.statistical.</code><code class="descname">bias_variance</code><span class="sig-paren">(</span><em>datasets=[]</em>, <em>algos=[]</em>, <em>metrics=[]</em>, <em>L=10</em>, <em>k=2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/statistical.html#bias_variance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.statistical.bias_variance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes the bias vs. variance decomposition of the error.
The approach used here is based on the works of <a class="reference internal" href="#webb05" id="id1">[Webb05]</a> and <a class="reference internal" href="#dom05" id="id2">[Dom05]</a>.</p>
<p>Each instance of the dataset is scored <cite>L</cite> times.
A single scoring is achieved by splitting the dataset at random into
<cite>k</cite> folds. Each fold is scored by the model <cite>M</cite> trained on the remainder folds.
<a class="reference internal" href="#webb05" id="id3">[Webb05]</a> recommends the use of 2 folds.</p>
<p>If metric is MSE then the standard decomposition is used.
The Bias for and instance <cite>x</cite> is defined as mean squared error of the <cite>L</cite> trained models
w.r.t. the true label <cite>y</cite>, denoted with <img class="math" src="_images/math/b9ea44c58866a76eb92b49412c50f3c65a46a2b2.png" alt="{\sf E}_{L} [M(x) - y]^2"/>. 
The Variance for an instance <cite>x</cite> is measured across the <cite>L</cite> trained models: 
<img class="math" src="_images/math/26259f1633b69e398903730e291a1379eec214f9.png" alt="{\sf E}_{L} [M(x) - {\sf E}_{L} M(x)]^2"/>. 
Both are averaged over all instances in the dataset.</p>
<p>If metric is any of the IR quality measures, we resort to the bias variance
decomposition of the mean squared error of the given metric w.r.t. its ideal value,
e.g., for the case of NDCG, <img class="math" src="_images/math/bfd36e8f29e5736675d8efaa6e585db4aaa65cd6.png" alt="{\sf E}_{L} [1 - NDCG]^2"/>. 
Recall that, a formal Bias/Variance decomposition was not proposed yet.</p>
<dl class="docutils">
<dt>dataset <span class="classifier-delimiter">:</span> <span class="classifier">rankeval.dataset.Dataset</span></dt>
<dd>The dataset instance.</dd>
<dt>algo <span class="classifier-delimiter">:</span> <span class="classifier">function</span></dt>
<dd><p class="first">This should be a wrapper of learning algorithm.
The function should accept four parameters: <cite>train_X</cite>, <cite>train_Y</cite>, <cite>train_q</cite>, <cite>test_X</cite>.</p>
<blockquote>
<div><ul class="simple">
<li><cite>train_X</cite>: numpy.ndarray storing a 2-D matrix of size num_docs x num_features</li>
<li><cite>train_Y</cite>: numpy.ndarray storing a vector of document’s relevance labels</li>
<li><cite>train_q</cite>: numpy.ndarray storing a vector of query lengths</li>
<li><cite>test_X</cite>: numpy.ndarray as for <cite>train_X</cite></li>
</ul>
</div></blockquote>
<p class="last">A model is trained on <cite>train_X</cite>, <cite>train_Y</cite>, <cite>train_q</cite>, and used to score <cite>test_X</cite>.
An numpy.ndarray with such score must be returned.</p>
</dd>
<dt>metric <span class="classifier-delimiter">:</span> <span class="classifier">“mse” or rankeval.metrics.metric.Metric</span></dt>
<dd>The metric used to compute the error.</dd>
<dt>L <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of iterations</dd>
<dt>k <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of folds.</dd>
</dl>
<dl class="docutils">
<dt>bias_variance <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the bias/variance decomposition of the error
for any given dataset, algorithm and metric.</dd>
</dl>
<table class="docutils citation" frame="void" id="webb05" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[Webb05]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id3">2</a>)</em> Webb, Geoffrey I., and Paul Conilione. “Estimating bias and variance from data.” 
Pre-publication manuscript (<a class="reference external" href="http://www.csse.monash.edu/webb/-Files/WebbConilione06.pdf">pdf</a>) (2005).</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="dom05" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[Dom05]</a></td><td>Domingos P. A unified bias-variance decomposition. 
In Proceedings of 17th International Conference on Machine Learning 2000 (pp. 231-238).</td></tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.statistical.statistical_significance">
<code class="descclassname">rankeval.analysis.statistical.</code><code class="descname">statistical_significance</code><span class="sig-paren">(</span><em>datasets</em>, <em>model_a</em>, <em>model_b</em>, <em>metrics</em>, <em>n_perm=100000</em>, <em>cache=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/statistical.html#statistical_significance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.statistical.statistical_significance" title="Permalink to this definition">¶</a></dt>
<dd><p>This method computes the statistical significance of the performance difference between model_a and.</p>
<dl class="docutils">
<dt>datasets <span class="classifier-delimiter">:</span> <span class="classifier">list of Dataset</span></dt>
<dd>The datasets to use for analyzing the behaviour of the model using the given metrics and models</dd>
<dt>model_a <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>The first model considered.</dd>
<dt>model_b <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>The second model considered.</dd>
<dt>metrics <span class="classifier-delimiter">:</span> <span class="classifier">list of Metric</span></dt>
<dd>The metrics to use for the analysis</dd>
<dt>n_perm <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of permutations for the randomization test.</dd>
<dt>cache <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether to cache or not the intermediate scoring results of each model
on each dataset. Caching enable instant access to the scores (after the
first scoring) but coudl make use also of a huge quantity of memory.</dd>
</dl>
<dl class="docutils">
<dt>stat_sig <span class="classifier-delimiter">:</span> <span class="classifier">xarray.DataArray</span></dt>
<dd>A DataArray containing the statistical significance of the performance difference
between any pair of models on the given dataset.</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-rankeval.analysis.topological">
<span id="rankeval-analysis-topological-module"></span><h2>rankeval.analysis.topological module<a class="headerlink" href="#module-rankeval.analysis.topological" title="Permalink to this headline">¶</a></h2>
<p>This package implements several topological analysis focused on the
topological characteristics of ensemble-based LtR models. These
functionalities can be applied to several models,
so as to have a direct comparison of the shape of the resulting
forests (e.g., trained by different LtR algorithms).</p>
<dl class="class">
<dt id="rankeval.analysis.topological.TopologicalAnalysisResult">
<em class="property">class </em><code class="descclassname">rankeval.analysis.topological.</code><code class="descname">TopologicalAnalysisResult</code><span class="sig-paren">(</span><em>model</em>, <em>include_leaves</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/topological.html#TopologicalAnalysisResult"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.topological.TopologicalAnalysisResult" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>This class is used to return the topological analysis made on the model.
Several low-level information are stored in this class, and then
re-elaborated to provide high-level analysis.</p>
<p>Analyze the model in a topological perspective</p>
<dl class="docutils">
<dt>model <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>the model to analyze from the topological perspective</dd>
<dt>include_leaves <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the leaves has to be included in the analysis or not</dd>
</dl>
<dl class="docutils">
<dt>model <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>The model analyzed</dd>
<dt>height_trees <span class="classifier-delimiter">:</span> <span class="classifier">numpy array</span></dt>
<dd>The ordered height of each trees composing the ensemble</dd>
<dt>topology <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse.csr_matrix</span></dt>
<dd>The matrix used to store low-level information related to the
aggregated shape of the trees. Each matrix cell identifies a
tree node with a pair of coordinates row-col, with row
highlighting the depth and col the column with respect
to a full binary tree.</dd>
</dl>
<dl class="method">
<dt id="rankeval.analysis.topological.TopologicalAnalysisResult.avg_tree_shape">
<code class="descname">avg_tree_shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/topological.html#TopologicalAnalysisResult.avg_tree_shape"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.topological.TopologicalAnalysisResult.avg_tree_shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the fraction of trees having each node with respect to a
full binary tree. The fraction is obtained by normalizing the count
by the number of trees composing the ensemble model.</p>
<dl class="docutils">
<dt>fractions <span class="classifier-delimiter">:</span> <span class="classifier">scipy.sparse.csr_matrix</span></dt>
<dd>Sparse matrix with the same shape of the topology matrix, where
each matrix cell identifies a tree node by a pair of coordinates
row-col, with row highlighting the depth and col the column with
respect to a full binary tree. Each cell value highlights how many
trees have the specific node, normalized by the number of trees.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rankeval.analysis.topological.TopologicalAnalysisResult.describe_tree_height">
<code class="descname">describe_tree_height</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/topological.html#TopologicalAnalysisResult.describe_tree_height"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.topological.TopologicalAnalysisResult.describe_tree_height" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes several descriptive statistics of the height of the trees.</p>
<dl class="docutils">
<dt>nobs <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of trees</dd>
<dt>minmax: tuple of ndarrays or floats</dt>
<dd>Minimum and maximum height of trees</dd>
<dt>mean <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or float</span></dt>
<dd>Arithmetic mean of tree heights.</dd>
<dt>variance <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or float</span></dt>
<dd>Unbiased variance of the tree heights.
denominator is number of trees minus one.</dd>
<dt>skewness <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or float</span></dt>
<dd>Skewness, based on moment calculations with denominator equal to
the number of trees, i.e. no degrees of freedom correction.</dd>
<dt>kurtosis <span class="classifier-delimiter">:</span> <span class="classifier">ndarray or float</span></dt>
<dd>Kurtosis (Fisher).  The kurtosis is normalized so that it is
zero for the normal distribution.  No degrees of freedom are used.</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="rankeval.analysis.topological.TopologicalAnalysisResult.fullness_per_level">
<code class="descname">fullness_per_level</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/topological.html#TopologicalAnalysisResult.fullness_per_level"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.topological.TopologicalAnalysisResult.fullness_per_level" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the normalized number of trees with full level i, for each
level of a full binary tree. The normalization is done by the number
of trees.</p>
<dl class="docutils">
<dt>fullness <span class="classifier-delimiter">:</span> <span class="classifier">np.array</span></dt>
<dd>An array long as the maximum height of a tree in the ensemble, and
where the j-th cell highlight how much the j-th level of the trees
is full (normalized by the number of trees).</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="rankeval.analysis.topological.topological_analysis">
<code class="descclassname">rankeval.analysis.topological.</code><code class="descname">topological_analysis</code><span class="sig-paren">(</span><em>model</em>, <em>include_leaves=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/rankeval/analysis/topological.html#topological_analysis"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#rankeval.analysis.topological.topological_analysis" title="Permalink to this definition">¶</a></dt>
<dd><p>This method implements the topological analysis of a ensemble-based
LtR model. Given a model, it studies the shape of each tree composing
the model and return several information useful for having insights
about the shape of the trees, their completeness (level by level) as
well as min/max/mean height and the fraction of trees having a specific
node (where each node is identified by a pair of coordinates row-col,
with row highlighting the depth and col the column with respect to a
full binary tree).</p>
<dl class="docutils">
<dt>model <span class="classifier-delimiter">:</span> <span class="classifier">RTEnsemble</span></dt>
<dd>The model to analyze</dd>
<dt>include_leaves <span class="classifier-delimiter">:</span> <span class="classifier">bool</span></dt>
<dd>Whether the leaves has to be included in the analysis or not</dd>
</dl>
<dl class="docutils">
<dt>object <span class="classifier-delimiter">:</span> <span class="classifier">TopologicalAnalysisResult</span></dt>
<dd>The topological result, to use for retrieving several information</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="rankeval.dataset.html" class="btn btn-neutral float-right" title="rankeval.dataset package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="rankeval.html" class="btn btn-neutral" title="rankeval package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, HPC Lab.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.00',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>