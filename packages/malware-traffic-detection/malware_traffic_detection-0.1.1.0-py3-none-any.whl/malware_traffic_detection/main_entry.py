"""

    neural network (autoencoder) to detect malware traffic
"""
# reproducitive setting
from utilities.random_control import *

from sklearn.model_selection import train_test_split

from AE_Model import AE_Model
from pcap2input.flows2features import load_data_from_flows_txt
from utilities.data_preprocessing import normalize_data, save_data, load_data
import os

from utilities.show_data import show_data_scatter, show_data_line
import numpy as np


def main():
    norm_flg = True  # normalize data
    fixed_size = 30
    Epoches = 50

    in_dirs_dict = {
        'unb_norm': 'data/UNB-IDS2017/Normal_traffic/Monday-WorkingHours_output_00001_20170703081536',
        'unb_malic': 'data/UNB-IDS2017/Malware_traffic/Friday-WorkingHours_output_time_10-05-00_10-20-00',
        'ctu_malic': 'data/CTU_malware_traffic/botnet-capture-20110810-neris-30000pkts',
        'skype_norm': 'data/Skype'
    }

    case = "U_norm-U_malic:L7"  # case[1]= UNB normal, case[3]= UNB malicious, case[5]=7 (L7)
    """ 
    case = "U_norm-U_malic:L7": it cannot be used in payload based experiment.
    
        Get all attack packets on Friday:  Friday Morning Botnet ARES (10:02 a.m. â€“ 11:02 a.m.)
            https://www.unb.ca/cic/datasets/ids-2017.html
            Attacker: Kali, 205.174.165.73 
            Victims: Win 10, 192.168.10.15 + Win 7, 192.168.10.9 + Win 10, 192.168.10.14 + Win 8, 192.168.10.5 + Vista, 192.168.10.8
        
        # step 1. only extract packets of 205.174.165.73 
        >> tshark -r Friday-WorkingHours.pcap -w output_filter_ips.pcap ip.addr==205.174.165.73
        # step 2. only extract the packets during the attack time  (10.02.00 to 11:02:00)
        >> editcap -F libpcap -A "2017-07-07 10:02:00" -B "2017-07-07 12:02:00" Friday-WorkingHours_output_filter_ip_205.174.165.73.pcap Friday-WorkingHours_output_filter_ip_205.174.165.73-attack_time-10:02:00-11:02:00.pcap
        # step 3. check the pcap by wireshark, found that all the packets are handshare packets, which means this pcap (Friday) cannot be used in our approach because it does not have any payload of attack packets.

    """

    case = "U_norm-CTU_malic:L7"
    # case = "Skype_norm-CTU_malic:L7"
    # case = "Skype_norm-U_malic:L7"
    if case == "U_norm-U_malic:L7":  # only TCP/UDP payload
        in_dir_lst = [in_dirs_dict['unb_norm'], in_dirs_dict['unb_malic']]
        storage_mode = "L7"
    elif case == "U_norm-U_malic:L4":  # TCP/UDP Header + payload, i.e., IP.payload
        in_dir_lst = [in_dirs_dict['unb_norm'], in_dirs_dict['unb_malic']]
        storage_mode = "L4"
    elif case == "U_norm-CTU_malic:L7":  # TCP/UDP Header + payload, i.e., IP.payload
        in_dir_lst = [in_dirs_dict['unb_norm'], in_dirs_dict['ctu_malic']]
        storage_mode = "L7"
    elif case == "Skype_norm-U_malic:L7":  # TCP/UDP Header + payload, i.e., IP.payload
        in_dir_lst = [in_dirs_dict['skype_norm'], in_dirs_dict['ctu_malic']]
        storage_mode = "L7"
    elif case == "Skype_norm-CTU_malic:L7":  # TCP/UDP Header + payload, i.e., IP.payload
        in_dir_lst = [in_dirs_dict['skype_norm'], in_dirs_dict['ctu_malic']]
        storage_mode = "L7"
    else:
        print('input right case.')
        return -1

    # load normal flows
    X = load_data_from_flows_txt(in_dir_lst[0], fixed_size)
    print(X[0])
    Y = [1] * len(X)  # 1 : normal, 0: attack
    print(f'X.shape ={X.shape}')
    x_train_tmp, x_test, y_train_tmp, y_test = train_test_split(X, Y, test_size=0.2)  # defalut: test_size =0.25
    x_train, x_val, y_train, y_val = train_test_split(x_train_tmp, y_train_tmp, test_size=0.125)
    print(f'x_train={x_train.shape},x_val={x_val.shape},x_test={x_test.shape}')

    # load malware flows
    x_malicious_test = load_data_from_flows_txt(in_dir_lst[1], fixed_size)
    x_malicious_test = x_malicious_test[:x_test.shape[0], :]  # only use the first 100 flows.
    print(f'x_malicious_test.shape:{x_malicious_test.shape}')
    y_malicious_test = [0] * len(x_malicious_test)  # 1 : normal, 0: malicious
    # x_test = x_malicious_test
    # y_test = y_malicious_test
    x_test = np.concatenate((x_test, x_malicious_test), axis=0)
    print(f'x_test:{x_test.shape}')
    y_test.extend(y_malicious_test)  # concatenate list.

    if norm_flg:
        x_train, x_train_scaler = normalize_data(x_train, mode='minmax')
        x_val = x_train_scaler.transform(x_val)
        x_test = x_train_scaler.transform(x_test)

    AE = AE_Model(in_dim=x_train.shape[1], Epoches=Epoches)
    res_dict, reconstr_thres = AE.train((x_train, y_train), (x_val, y_val))
    out_file = os.path.join(os.path.dirname(in_dir_lst[0]), 'train_val_results.txt')
    save_data(res_dict, out_file)

    data_dict = load_data(out_file)
    show_data_line(data_dict['loss_train'], data_dict['loss_val'], ax_labels=['Epoches', 'Loss'],
                   labels=['loss_train', 'loss_val'])

    reconstr_thres = 0.08  # manually setting.
    print(f'Evaluating..., reconstr_thres={reconstr_thres}')
    loss_test, cm_test, dists_test = AE.evaluate((x_test, y_test), reconstr_thres=reconstr_thres)
    print(f'cm_test:{cm_test},\nloss_test={loss_test}')

    show_data_line(dists_test['normal'], dists_test['malicious'], ax_labels=['Number', 'Distance'],
                   labels=['normal', 'malicious'])
    y = [1] * len(dists_test['normal']) + [0] * len(dists_test['malicious'])
    show_data_scatter(dists_test['normal'], dists_test['malicious'], y, ax_labels=['Number', 'Distance'],
                      labels=['normal', 'malicious'])


if __name__ == '__main__':
    main()
